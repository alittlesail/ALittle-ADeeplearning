
namespace ADeeplearning;

public class IARobotDqn
{
    public fun Copy(IARobotDqn dnn) { }
    public fun Calc(ARobotExpression input) : ARobotExpression { return null; }
}

public enum ARobotDqnTypes
{
    NORMAL = 1;
    DUELING = 2;
    DOUBLE = 3;
}

public class ARobotDqnModel
{
    private carp.CarpRobotSumTree _sum_tree;
    private IARobotDqn _eval_net;
    private IARobotDqn _target_net;

    private ARobotInput _state;
    private ARobotInput _next_state;
    private ARobotInput _reward;
    private ARobotLabel _action;
    private ARobotInput _target;

    private ARobotExpression _out;
    private ARobotExpression _loss;
    private ARobotExpression _q_eval;
    private ARobotExpression _q_next;
    private ARobotExpression _target_q_next;

    private int _learn_step_counter = 0;
    private ARobotSession _session;

    private int _model_type;

    private fun InitGraph(int action_count, int memory_capacity)
    {
        this._sum_tree = new carp.CarpRobotSumTree(memory_capacity);
        this._reward = this._session.CreateInput([1]);
        this._target = this._session.CreateInput([1]);
        this._action = this._session.CreateLabel();

        // 输入参数准备
        var reward = this._reward.Calc();
        var state = this._state.Calc();
        var target = this._target.Calc();
        var next_state = this._next_state.Calc();

        // 针对做过的动作action, 来选 q_eval 的值, (q_eval 原本有所有动作的值)
        this._out = this._eval_net.Calc(state);
        this._q_eval = this._out.PickElement(this._action, 0);
        // q估计
        this._target_q_next = this._target_net.Calc(next_state);

        if (this._model_type == ARobotDqnTypes.DOUBLE)
            this._q_next = this._eval_net.Calc(next_state);

        // 计算q现实，和q估计的loss
        this._loss = this._q_eval.Subtraction(target).Square();
    }

    public fun Load(string file_path) { this._session.Load(file_path); }
    public fun Save([Nullable]string file_path) { this._session.Save(file_path); }

    public fun Train(int count) : double
    {
		var index_list = this._sum_tree.SelectMemory(count);
        var index_count = ALittle.List_Len(index_list);
        if (index_count == 0) return 0;
		
    	var total_loss = 0;
    	for (var _, index in index_list)
    	{
            this._session.Reset();

            if (this._learn_step_counter % 100 == 0)
                this._target_net.Copy(this._eval_net);
            this._learn_step_counter += 1;

            // 准备数据
            var reward = this._sum_tree.GetReward(index, this._reward.GetInput());
            this._sum_tree.GetState(index, this._state.GetInput());
            this._sum_tree.GetNextState(index, this._next_state.GetInput());
            var action = this._sum_tree.GetAction(index, this._action.GetLabel());
        
            if (this._model_type == ARobotDqnTypes.DOUBLE)
            {
                var next_action = this._target_q_next.AsVectorAndArgmax();
                var q_target = reward + 0.9 * this._q_next.AsVectorAndGetValue(next_action);
                this._target.Update([q_target]);
            }
            else
            {
                var q_target = reward + 0.9 * this._target_q_next.AsVectorAndMaxValue();
                this._target.Update([q_target]);
            }

    		var loss = this._loss.AsScalar();

            this._session.Train();
    		total_loss += loss;
    	}

        return total_loss / index_count;
    }

    public fun SaveTransition(List<double> state, List<double> next_state, int action, double reward) : bool
    {
        this._session.Reset();

        this._reward.Update([reward]);
        this._state.Update(state);
        this._next_state.Update(next_state);
        this._action.Update(action);
    
        var q_target = reward + 0.9 * this._q_next.AsVectorAndMaxValue();
        this._target.Update([q_target]);

        var loss = this._loss.AsScalar();

        return this._sum_tree.SaveMemory(state, next_state, action, reward, loss);
    }

    public fun ChooseAction(List<double> state) : int
    {
        this._session.Reset();
	    this._state.Update(state);
        return this._out.AsVectorAndArgmax();
    }
}

public class ARobotDuelingDqnDnn : IARobotDqn
{
    private ARobotLinear _linear;
    private ARobotLinear _value;
    private ARobotLinear _advantage;

    public ctor(ARobotSession session, int state_count, int action_count, int hide_dim)
    {
        this._linear = session.CreateLinear(state_count, hide_dim);
        this._value = session.CreateLinear(hide_dim, 1);
        this._advantage = session.CreateLinear(hide_dim, action_count);
    }

    public fun Copy(IARobotDqn dnn)
    {
        var value = ALittle.Cast{ARobotDuelingDqnDnn}(dnn);

        this._linear.Copy(value._linear);
        this._value.Copy(value._value);
        this._advantage.Copy(value._advantage);
    }

    public fun Calc(ARobotExpression input) : ARobotExpression
    {
		// 全连接隐藏层
		var x = this._linear.Calc(input).Rectify();

		// 分析 state 的 value
		var value = this._value.Calc(x);

		// 专门分析每种动作的 Advantage
		var advantage = this._advantage.Calc(x);

		// 合并 V 和 A, 为了不让 A 直接学成了 Q, 我们减掉了 A 的均值
		return advantage.Subtraction(advantage.MeanElements(0)).Addition(value);
    }
}

public class ARobotDqnDnn : IARobotDqn
{
    private ARobotLinear _fc1;
    private ARobotLinear _fc2;

    public ctor(ARobotSession session, int state_count, int action_count, int hide_dim)
    {
        this._fc1 = session.CreateLinear(state_count, hide_dim);
        this._fc2 = session.CreateLinear(hide_dim, action_count);
    }

    public fun Copy(IARobotDqn dnn)
    {
        var value = ALittle.Cast{ARobotDqnDnn}(dnn);

        this._fc1.Copy(value._fc1);
        this._fc2.Copy(value._fc2);
    }

    public fun Calc(ARobotExpression input) : ARobotExpression
    {
		// 全连接隐藏层
		var x = this._fc1.Calc(input).Rectify();
		return this._fc2.Calc(x);
    }
}

public class ARobotDqnDnnModel : ARobotDqnModel
{
    public ctor(int state_count, int action_count, int hide_dim, int memory_capacity, [Nullable] int type)
    {
        this._model_type = type;
        this._session = new ARobotSession();
        this._state = this._session.CreateInput([state_count]);
        this._next_state = this._session.CreateInput([state_count]);
 
        if (type == ARobotDqnTypes.DUELING)
        {
            this._eval_net = new ARobotDuelingDqnDnn(this._session, state_count, action_count, hide_dim);
            this._target_net = new ARobotDuelingDqnDnn(this._session, state_count, action_count, hide_dim);
        }
        else
        {
            this._eval_net = new ARobotDqnDnn(this._session, state_count, action_count, hide_dim);
            this._target_net = new ARobotDqnDnn(this._session, state_count, action_count, hide_dim);
        }

        this.InitGraph(action_count, memory_capacity);
    }
}

public class ARobotDuelingDqnCnn : IARobotDqn
{
    private List<ARobotConv2D> _conv2d_list;
    private ARobotLinear _linear;
    private ARobotLinear _value;
    private ARobotLinear _advantage;

    private int _reshapre = 0;

    public ctor(ARobotSession session, int input_width, int input_height, int action_count, List<int> conv2d_dim_list, int linear_dim)
    {
        this._conv2d_list = new List<ARobotConv2D>();
        var last_input_dim = 1;
        for (var index, dim in conv2d_dim_list)
        {
            var conv2d = session.CreateConv2D(last_input_dim, dim, 2, 2, 1, 1, false);
            this._conv2d_list[index] = conv2d;
            last_input_dim = dim;
        }
        this._reshapre = last_input_dim * input_width * input_height;
        this._linear = session.CreateLinear(this._reshapre, linear_dim);
        this._value = session.CreateLinear(linear_dim, 1);
        this._advantage = session.CreateLinear(linear_dim, action_count);
    }

    public fun Copy(IARobotDqn cnn)
    {
        var value = ALittle.Cast{ARobotDuelingDqnCnn}(cnn);

        for (var index, conv2d in this._conv2d_list)
        	this._conv2d_list[index].Copy(value._conv2d_list[index]);
        this._linear.Copy(value._linear);
        this._value.Copy(value._value);
        this._advantage.Copy(value._advantage);
    }

    public fun Calc(ARobotExpression input) : ARobotExpression
    {
        // 卷积层
        var x = input;
        for (var index, conv2d in this._conv2d_list)
        	x = conv2d.Calc(x).Rectify();
        // 转为向量
        x = x.Reshape([this._reshapre]);
		// 全连接隐藏层
		x = this._linear.Calc(x).Rectify();

		// 分析 state 的 value
		var value = this._value.Calc(x);

		// 专门分析每种动作的 Advantage
		var advantage = this._advantage.Calc(x);

		// 合并 V 和 A, 为了不让 A 直接学成了 Q, 我们减掉了 A 的均值
		return advantage.Subtraction(advantage.MeanElements(0)).Addition(value);
    }
}

public class ARobotDqnCnn : IARobotDqn
{
    private List<ARobotConv2D> _conv2d_list;
    private ARobotLinear _fc_1;
    private ARobotLinear _fc_2;

    private int _reshapre = 0;

    public ctor(ARobotSession session, int input_width, int input_height, int action_count, List<int> conv2d_dim_list, int linear_dim)
    {
        this._conv2d_list = new List<ARobotConv2D>();
        var last_input_dim = 1;
        for (var index, dim in conv2d_dim_list)
        {
            var conv2d = session.CreateConv2D(last_input_dim, dim, 2, 2, 1, 1, false);
            this._conv2d_list[index] = conv2d;
            last_input_dim = dim;
        }
        this._reshapre = last_input_dim * input_width * input_height;
        this._fc_1 = session.CreateLinear(this._reshapre, linear_dim);
        this._fc_2 = session.CreateLinear(linear_dim, action_count);
    }

    public fun Copy(IARobotDqn cnn)
    {
        var value = ALittle.Cast{ARobotDqnCnn}(cnn);

        for (var index, conv2d in this._conv2d_list)
        	this._conv2d_list[index].Copy(value._conv2d_list[index]);
        this._fc_1.Copy(value._fc_1);
        this._fc_2.Copy(value._fc_2);
    }

    public fun Calc(ARobotExpression input) : ARobotExpression
    {
        // 卷积层
        var x = input;
        for (var index, conv2d in this._conv2d_list)
        	x = conv2d.Calc(x).Rectify();
        x = x.Reshape([this._reshapre]);

		// 全连接隐藏层
		x = this._fc_1.Calc(x).Rectify();
		return this._fc_2.Calc(x);
    }
}

public class ARobotDqnCnnModel : ARobotDqnModel
{
    public ctor(int input_width, int input_height, int action_count, List<int> conv2d_dim_list, int linear_dim, int memory_capacity, [Nullable] int type)
    {
        this._model_type = type;
        this._session = new ARobotSession();
        this._state = this._session.CreateInput([input_width, input_height, 1]);
        this._next_state = this._session.CreateInput([input_width, input_height, 1]);
    
        if (type == ARobotDqnTypes.DUELING)
        {
            this._eval_net = new ARobotDuelingDqnCnn(this._session, input_width, input_height, action_count, conv2d_dim_list, linear_dim);
            this._target_net = new ARobotDuelingDqnCnn(this._session, input_width, input_height, action_count, conv2d_dim_list, linear_dim);
        }
        else
        {
            this._eval_net = new ARobotDqnCnn(this._session, input_width, input_height, action_count, conv2d_dim_list, linear_dim);
            this._target_net = new ARobotDqnCnn(this._session, input_width, input_height, action_count, conv2d_dim_list, linear_dim);
        }

        this.InitGraph(action_count, memory_capacity);
    }
}