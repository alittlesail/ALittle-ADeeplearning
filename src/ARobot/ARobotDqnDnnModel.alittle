
namespace ADeeplearning;

public class IARobotDqnDnn
{
    public ctor(ARobotSession session, int state_count, int action_count, int hide_dim) { }
    public fun Copy(IARobotDqnDnn dnn) { }
    public fun Calc(ARobotExpression input) : ARobotExpression { return null; }
}

public class ARobotDuelingDqnDnn : IARobotDqnDnn
{
    private ARobotLinear _linear;
    private ARobotLinear _value;
    private ARobotLinear _advantage;

    public ctor(ARobotSession session, int state_count, int action_count, int hide_dim)
    {
        this._linear = session.CreateLinear(state_count, hide_dim);
        this._value = session.CreateLinear(hide_dim, 1);
        this._advantage = session.CreateLinear(hide_dim, action_count);
    }

    public fun Copy(IARobotDqnDnn dnn)
    {
        var value = ALittle.Cast{ARobotDuelingDqnDnn}(dnn);

        this._linear.Copy(value._linear);
        this._value.Copy(value._value);
        this._advantage.Copy(value._advantage);
    }

    public fun Calc(ARobotExpression input) : ARobotExpression
    {
		// 全连接隐藏层
		var x = this._linear.Calc(input);
		x = x.Rectify();

		// 分析 state 的 value
		var value = this._value.Calc(x);

		// 专门分析每种动作的 Advantage
		var advantage = this._advantage.Calc(x);

		// 合并 V 和 A, 为了不让 A 直接学成了 Q, 我们减掉了 A 的均值
		return advantage.Subtraction(advantage.MeanElements(0)).Addition(value);
    }
}

public class ARobotDqnDnn : IARobotDqnDnn
{
    private ARobotLinear _fc1;
    private ARobotLinear _fc2;

    public ctor(ARobotSession session, int state_count, int action_count, int hide_dim)
    {
        this._fc1 = session.CreateLinear(state_count, hide_dim);
        this._fc2 = session.CreateLinear(hide_dim, action_count);
    }

    public fun Copy(IARobotDqnDnn dnn)
    {
        var value = ALittle.Cast{ARobotDqnDnn}(dnn);

        this._fc1.Copy(value._fc1);
        this._fc2.Copy(value._fc2);
    }

    public fun Calc(ARobotExpression input) : ARobotExpression
    {
		// 全连接隐藏层
		var x = this._fc1.Calc(input);
		x = x.Rectify();
		return this._fc2.Calc(x);
    }
}

public enum ARobotDqnTypes
{
    NORMAL = 1;
    DUELING = 2;
}

public class ARobotDqnDnnModel
{
    private carp.CarpRobotSumTree _sum_tree;
    private IARobotDqnDnn _eval_net;
    private IARobotDqnDnn _target_net;

    private ARobotInput _state;
    private ARobotInput _next_state;
    private ARobotInput _reward;
    private ARobotLabel _action;
    private ARobotInput _target;

    private ARobotExpression _out;
    private ARobotExpression _loss;
    private ARobotExpression _q_eval;
    private ARobotExpression _q_next;

    private int _state_count = 0;
    private int _action_count = 0;

    private int _learn_step_counter = 0;
    private ARobotSession _session;

    public ctor(int state_count, int action_count, int hide_dim, int memory_capacity, [Nullable] int type)
    {
        this._session = new ARobotSession();
        this._state_count = state_count;
        this._action_count = action_count;

        this._sum_tree = new carp.CarpRobotSumTree(memory_capacity);
        this._state = this._session.CreateInput([this._state_count]);
        this._next_state = this._session.CreateInput([this._state_count]);
        this._reward = this._session.CreateInput([1]);
        this._target = this._session.CreateInput([1]);
        this._action = this._session.CreateLabel();

        if (type == ARobotDqnTypes.DUELING)
        {
            this._eval_net = new ARobotDuelingDqnDnn(this._session, state_count, action_count, hide_dim);
            this._target_net = new ARobotDuelingDqnDnn(this._session, state_count, action_count, hide_dim);
        }
        else
        {
            this._eval_net = new ARobotDqnDnn(this._session, state_count, action_count, hide_dim);
            this._target_net = new ARobotDqnDnn(this._session, state_count, action_count, hide_dim);
        }

        // 输入参数准备
        var reward = this._reward.Calc();
        var state = this._state.Calc();
        var target = this._target.Calc();
        var next_state = this._next_state.Calc();

        // 针对做过的动作action, 来选 q_eval 的值, (q_eval 原本有所有动作的值)
        this._out = this._eval_net.Calc(state);
        this._q_eval = this._out.PickElement(this._action, 0);

        // q估计
        this._q_next = this._target_net.Calc(next_state);

        // 计算q现实，和q估计的loss
        this._loss = this._q_eval.Subtraction(target).Square();
    }

    public fun Load(string file_path) { this._session.Load(file_path); }
    public fun Save([Nullable]string file_path) { this._session.Save(file_path); }

    public fun Train(int count) : double
    {
		var index_list = this._sum_tree.SelectMemory(count);
        var index_count = ALittle.List_Len(index_list);
        if (index_count == 0) return 0;
		
    	var total_loss = 0;
    	for (var _, index in index_list)
    	{
            this._session.Reset();

            if (this._learn_step_counter % 100 == 0)
                this._target_net.Copy(this._eval_net);
            this._learn_step_counter += 1;

            // 准备数据
            var reward = this._sum_tree.GetReward(index, this._reward.GetInput());
            this._sum_tree.GetState(index, this._state.GetInput());
            this._sum_tree.GetNextState(index, this._next_state.GetInput());
            var action = this._sum_tree.GetAction(index, this._action.GetLabel());
        
            // 准备目标值
    		var q_target = reward + 0.9 * this._q_next.AsVectorAndMaxValue();
            this._target.Update([q_target]);

    		var loss = this._loss.AsScalar();

            this._session.Train();
    		total_loss += loss;
    	}

        return total_loss / index_count;
    }

    public fun SaveTransition(List<double> state, List<double> next_state, int action, double reward)
    {
        this._session.Reset();

        this._reward.Update([reward]);
        this._state.Update(state);
        this._next_state.Update(next_state);
        this._action.Update(action);
    
        var q_target = reward + 0.9 * this._q_next.AsVectorAndMaxValue();
        this._target.Update([q_target]);

        var loss = this._loss.AsScalar();

        this._sum_tree.SaveMemory(state, next_state, action, reward, loss);
    }

    public fun ChooseAction(List<double> state) : int
    {
        this._session.Reset();
	    this._state.Update(state);
        return this._out.AsVectorAndArgmax();
    }
}